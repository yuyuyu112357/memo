tags:
	#literature 
	#refactoring 
	#file-ops 
	#design-pattern 
	#oop
	#fp
source:
	[[Pythonにおけるファイル操作の設計パターン]]
	OpenAI o3

---
# Q&A
## Q1.
ファイル操作において、ストレージ種別を切り替え可能な実装は?

## A1.
複数のストレージ実装が想定される場合は Strategy / Adapter パターンで抽象化する。
具体的には、
* ストレージを抽象基底クラス `StorageBackend` で統一  
* ローカル実装 `LocalStorageBackend` と S3 実装 `S3StorageBackend` を用意  
* アプリ側は `FileManager` に任せるだけで、どのストレージを使うか意識しない
という「Strategy / Adapter」構成を最小限で示します。  
(※ S3 の実装は boto3 が入っている前提。細部は簡略化しています)

base.py
```python
from __future__ import annotations

import abc
from pathlib import Path
from typing import Iterable, Union


class StorageBackend(abc.ABC):
    """
    ファイル配置の実アルゴリズムを差し替えられる Strategy 兼
    boto3 / pathlib などライブラリ差異を吸収する Adapter 。
    """

    # --- ファイルの登録／移動 --------------------------------------------

    @abc.abstractmethod
    def put_to_work(self, src_path: Union[str, Path]) -> str:
        """任意ファイルを work 領域へ配置し、そのキー(パス)を返す"""

    @abc.abstractmethod
    def move_to_result(self, work_key: str) -> str:
        """work 領域上のキーを result 領域へ移動し、移動後のキーを返す"""

    # --- 列挙 -------------------------------------------------------------

    @abc.abstractmethod
    def list_work(self, pattern: str = "*") -> Iterable[str]:
        """work 領域の一覧"""

    @abc.abstractmethod
    def list_result(self, pattern: str = "*") -> Iterable[str]:
        """result 領域の一覧"""

    # --- クリーニング ------------------------------------------------------

    @abc.abstractmethod
    def clean_work(self) -> None:
        ...

    @abc.abstractmethod
    def clean_result(self) -> None:
        ...

```

local.py
```python
from __future__ import annotations

import shutil
from pathlib import Path
from typing import Iterable, Union

from .base import StorageBackend


class LocalStorageBackend(StorageBackend):
    """
    ローカルディレクトリを使った実装
    """

    def __init__(self, base_dir: Union[str, Path]) -> None:
        base = Path(base_dir).expanduser().resolve()
        self.work_dir = base / "work"
        self.result_dir = base / "result"
        self.work_dir.mkdir(parents=True, exist_ok=True)
        self.result_dir.mkdir(parents=True, exist_ok=True)

    # -------- Strategy 本体 ----------------------------------------------

    def put_to_work(self, src_path: Union[str, Path]) -> str:
        src = Path(src_path).resolve()
        dst = self.work_dir / src.name
        shutil.copy2(src, dst)
        return dst.as_posix()

    def move_to_result(self, work_key: str) -> str:
        src = Path(work_key)
        dst = self.result_dir / src.name
        shutil.move(src.as_posix(), dst.as_posix())
        return dst.as_posix()

    # -------- 列挙 --------------------------------------------------------

    def list_work(self, pattern: str = "*") -> Iterable[str]:
        for p in self.work_dir.glob(pattern):
            yield p.as_posix()

    def list_result(self, pattern: str = "*") -> Iterable[str]:
        for p in self.result_dir.glob(pattern):
            yield p.as_posix()

    # -------- クリーニング --------------------------------------------------

    def clean_work(self) -> None:
        self._rimraf(self.work_dir)

    def clean_result(self) -> None:
        self._rimraf(self.result_dir)

    # -------- private ------------------------------------------------------

    @staticmethod
    def _rimraf(target: Path) -> None:
        for p in target.iterdir():
            if p.is_file():
                p.unlink()
            else:
                shutil.rmtree(p)

```

s3.py
```python
from __future__ import annotations

import mimetypes
from pathlib import Path
from typing import Iterable, Union

import boto3

from .base import StorageBackend


class S3StorageBackend(StorageBackend):
    """
    S3 バケットを使った実装 (簡易版)
    work/ と result/ をバケット内のプレフィクスで表現。
    """

    def __init__(self, bucket: str, prefix: str = "", s3_client=None) -> None:
        self.bucket = bucket
        # 末尾スラッシュは付けない
        self.prefix = prefix.rstrip("/")
        self.s3 = s3_client or boto3.client("s3")

    # -------- Strategy 本体 ----------------------------------------------

    def put_to_work(self, src_path: Union[str, Path]) -> str:
        src = Path(src_path)
        key = self._key("work", src.name)
        content_type, _ = mimetypes.guess_type(src.name)
        self.s3.upload_file(
            Filename=str(src),
            Bucket=self.bucket,
            Key=key,
            ExtraArgs={"ContentType": content_type} if content_type else None,
        )
        return key

    def move_to_result(self, work_key: str) -> str:
        result_key = work_key.replace("/work/", "/result/", 1)
        # S3 copy + delete で MOVE 相当
        self.s3.copy_object(
            Bucket=self.bucket, CopySource={"Bucket": self.bucket, "Key": work_key}, Key=result_key
        )
        self.s3.delete_object(Bucket=self.bucket, Key=work_key)
        return result_key

    # -------- 列挙 --------------------------------------------------------

    def list_work(self, pattern: str = "*") -> Iterable[str]:
        yield from self._list("work/")

    def list_result(self, pattern: str = "*") -> Iterable[str]:
        yield from self._list("result/")

    # -------- クリーニング --------------------------------------------------

    def clean_work(self) -> None:
        self._delete_prefix("work/")

    def clean_result(self) -> None:
        self._delete_prefix("result/")

    # -------- private ------------------------------------------------------

    def _key(self, area: str, filename: str) -> str:
        if self.prefix:
            return f"{self.prefix}/{area}/{filename}"
        return f"{area}/{filename}"

    def _list(self, area_prefix: str) -> Iterable[str]:
        prefix = f"{self.prefix}/{area_prefix}" if self.prefix else area_prefix
        paginator = self.s3.get_paginator("list_objects_v2")
        for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):
            for obj in page.get("Contents", []):
                yield obj["Key"]

    def _delete_prefix(self, area_prefix: str) -> None:
        prefix = f"{self.prefix}/{area_prefix}" if self.prefix else area_prefix
        # keys をまとめて削除 (バッチ処理簡略化)
        paginator = self.s3.get_paginator("list_objects_v2")
        keys = []
        for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):
            keys.extend([o["Key"] for o in page.get("Contents", [])])
        if keys:
            self.s3.delete_objects(
                Bucket=self.bucket,
                Delete={"Objects": [{"Key": k} for k in keys], "Quiet": True},
            )

```

file_manager.py
```python
from __future__ import annotations

from pathlib import Path
from typing import Union

from .storage.base import StorageBackend


class FileManager:
    """
    アプリが直接使う Facade。
    具体的ストレージを constructor injection することで
    Strategy を動的に差し替えられる。
    """

    def __init__(self, backend: StorageBackend) -> None:
        self.backend = backend

    # 以降は public API。実際の I/O は backend に委譲するだけ。

    def save_to_work(self, src_path: Union[str, Path]) -> str:
        return self.backend.put_to_work(src_path)

    def finalize(self, work_key: str) -> str:
        """
        処理済みファイルを result へ移動 (名前は move_to_result に合わせても良い)
        """
        return self.backend.move_to_result(work_key)

    def list_work(self):
        return list(self.backend.list_work())

    def list_result(self):
        return list(self.backend.list_result())

    def clean_all(self):
        self.backend.clean_work()
        self.backend.clean_result()

```

use_local.py
```python
from src.file_manager import FileManager
from src.storage.local import LocalStorageBackend

fm = FileManager(LocalStorageBackend(base_dir="./sandbox"))

key = fm.save_to_work("data/sample.csv")
print("work key:", key)

result_key = fm.finalize(key)
print("moved to:", result_key)

print("result files:", fm.list_result())

```

use_s3.py
```python
from src.file_manager import FileManager
from src.storage.s3 import S3StorageBackend

backend = S3StorageBackend(bucket="my-bucket", prefix="myapp")
fm = FileManager(backend)

key = fm.save_to_work("data/sample.csv")
print("uploaded to work:", key)

fm.finalize(key)
print("result objects on S3:", fm.list_result())

```

## Q2.
クラスを用いたファイル操作ではなく、関数型よりの設計はありませんか?

### A2.
「ファイル I/O をまとめた 1 つのクラス」を作らず、
1. “設定を表すただのデータ” を @dataclass で定義し
2. その設定とファイルパスだけを受け取る “状態を持たない関数” 群で I/O を行う

――という FP（関数型）寄りのスタイルも十分実践例があります。
Django の settings + 各種ユーティリティ関数や、標準ライブラリの os, shutil などは
まさにこの形です。

特に
* 「オブジェクトを生成させたくない」「DI で設定だけ差し替えたい」
* 「Pure function に近い形でテストしたい」

といったケースでは、クラスより関数 + dataclass の方が見通しがよくなることも多いです。

config.py
```python
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import boto3


# ---------- ローカル用設定 -----------------------------------------------


@dataclass(frozen=True)
class LocalConfig:
    base_dir: Path

    @property
    def work_dir(self) -> Path:
        return self.base_dir / "work"

    @property
    def result_dir(self) -> Path:
        return self.base_dir / "result"


# ---------- S3 用設定 -----------------------------------------------------


@dataclass(frozen=True)
class S3Config:
    bucket: str
    prefix: str = ""              # 例: "myapp"
    client: Optional["boto3.client"] = None  # 単体テスト用に stub を注入可

```

ops.py
```python
from __future__ import annotations

import mimetypes
import shutil
from functools import singledispatch
from pathlib import Path
from typing import Iterable

import boto3

from .config import LocalConfig, S3Config

# -------------------------------------------------------------------------
# 基本 API
# -------------------------------------------------------------------------

@singledispatch
def put_to_work(cfg, src: str | Path) -> str:  # noqa: ANN001
    raise NotImplementedError(f"Unsupported config type: {type(cfg)}")


@singledispatch
def move_to_result(cfg, work_key: str) -> str:  # noqa: ANN001
    raise NotImplementedError


@singledispatch
def list_work(cfg) -> Iterable[str]:  # noqa: ANN001
    raise NotImplementedError


@singledispatch
def list_result(cfg) -> Iterable[str]:  # noqa: ANN001
    raise NotImplementedError


# -------------------------------------------------------------------------
# LocalConfig の実装
# -------------------------------------------------------------------------

@put_to_work.register
def _(cfg: LocalConfig, src: str | Path) -> str:
    src = Path(src).resolve()
    cfg.work_dir.mkdir(parents=True, exist_ok=True)
    dst = cfg.work_dir / src.name
    shutil.copy2(src, dst)
    return dst.as_posix()


@move_to_result.register
def _(cfg: LocalConfig, work_key: str) -> str:
    src = Path(work_key)
    cfg.result_dir.mkdir(parents=True, exist_ok=True)
    dst = cfg.result_dir / src.name
    shutil.move(src, dst)
    return dst.as_posix()


@list_work.register
def _(cfg: LocalConfig) -> list[str]:
    return [p.as_posix() for p in cfg.work_dir.glob("*")]


@list_result.register
def _(cfg: LocalConfig) -> list[str]:
    return [p.as_posix() for p in cfg.result_dir.glob("*")]


# -------------------------------------------------------------------------
# S3Config の実装
# -------------------------------------------------------------------------

def _s3(cfg: S3Config):
    return cfg.client or boto3.client("s3")


def _prefix(cfg: S3Config, area: str) -> str:
    return f"{cfg.prefix}/{area}/" if cfg.prefix else f"{area}/"


@put_to_work.register
def _(cfg: S3Config, src: str | Path) -> str:
    s3 = _s3(cfg)
    src = Path(src)
    key = f"{_prefix(cfg, 'work')}{src.name}"
    ctype, _ = mimetypes.guess_type(src.name)
    s3.upload_file(
        Filename=str(src),
        Bucket=cfg.bucket,
        Key=key,
        ExtraArgs={"ContentType": ctype} if ctype else None,
    )
    return key


@move_to_result.register
def _(cfg: S3Config, work_key: str) -> str:
    s3 = _s3(cfg)
    result_key = work_key.replace("/work/", "/result/", 1)
    s3.copy_object(
        Bucket=cfg.bucket, CopySource={"Bucket": cfg.bucket, "Key": work_key}, Key=result_key
    )
    s3.delete_object(Bucket=cfg.bucket, Key=work_key)
    return result_key


def _list_s3_keys(cfg: S3Config, area: str) -> list[str]:
    s3 = _s3(cfg)
    pref = _prefix(cfg, area)
    paginator = s3.get_paginator("list_objects_v2")
    keys: list[str] = []
    for page in paginator.paginate(Bucket=cfg.bucket, Prefix=pref):
        keys.extend([c["Key"] for c in page.get("Contents", [])])
    return keys


@list_work.register
def _(cfg: S3Config) -> list[str]:
    return _list_s3_keys(cfg, "work")


@list_result.register
def _(cfg: S3Config) -> list[str]:
    return _list_s3_keys(cfg, "result")

```

functional_local.py
```python
from pathlib import Path

from src.config import LocalConfig
from src import ops

cfg = LocalConfig(base_dir=Path("./sandbox"))

key = ops.put_to_work(cfg, "data/sample.csv")
print("work key:", key)

ops.move_to_result(cfg, key)
print("result files ->", list(ops.list_result(cfg)))

```

functional_s3.py
```python
from src.config import S3Config
from src import ops

cfg = S3Config(bucket="my-bucket", prefix="myapp")

key = ops.put_to_work(cfg, "data/sample.csv")
print("uploaded:", key)

ops.move_to_result(cfg, key)
print("S3 result keys ->", list(ops.list_result(cfg)))

```

 メリット
1. 関数は “設定＋入力” だけを受け取り、副作用はファイル I/O のみ。テスト時は `tmp_path` や stub S3 クライアントを渡すだけで OK。
2. DI がシンプル（`cfg` を変えるだけ）で、状態を持たないぶん並列実行にも強い。
3. `singledispatch` やパターンマッチを使えば “型ごとの実装切替え” ＝ Strategy を関数レベルで表現できる。

デメリット
* 呼び出し側が毎回 `cfg` を渡す必要がある（`functools.partial` や closure で隠すことも可）。  
* インターフェースが「同じ関数名 + 型ディスパッチ」に依存するため、
  IDE 補完や型チェックは Protocol ベースの OOP より弱くなる場合がある。
